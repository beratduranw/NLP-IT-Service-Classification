{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b334307a",
   "metadata": {},
   "source": [
    "Read Data and Discover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29202de8ccca095f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Topic_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>connection with icon icon dear please setup ic...</td>\n",
       "      <td>Hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>work experience user work experience user hi w...</td>\n",
       "      <td>Access</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>requesting for meeting requesting meeting hi p...</td>\n",
       "      <td>Hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>reset passwords for external accounts re expir...</td>\n",
       "      <td>Access</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mail verification warning hi has got attached ...</td>\n",
       "      <td>Miscellaneous</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Document    Topic_group\n",
       "0  connection with icon icon dear please setup ic...       Hardware\n",
       "1  work experience user work experience user hi w...         Access\n",
       "2  requesting for meeting requesting meeting hi p...       Hardware\n",
       "3  reset passwords for external accounts re expir...         Access\n",
       "4  mail verification warning hi has got attached ...  Miscellaneous"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"all_tickets_processed_improved_v3.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b2c207",
   "metadata": {},
   "source": [
    "Remove Stopwords Using nltk Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a17078db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    words = text.lower().split()\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "df['Document'] = df['Document'].apply(remove_stopwords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beac99e",
   "metadata": {},
   "source": [
    "Creating Cloud To Observe The Most Used Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e66e0b7d8967d4d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_cloud(data_frame):\n",
    "    # Get the text from the Document column\n",
    "    text = data_frame[\"Document\"].str.cat(sep=\" \")  # Concatenate all documents\n",
    "    text = text.lower()\n",
    "\n",
    "    # Create the WordCloud object\n",
    "    wordcloud = WordCloud(max_font_size=40).generate(text)\n",
    "\n",
    "    # Create a plot and display the word cloud\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Most Frequent Words\")\n",
    "    plt.show()\n",
    "\n",
    "#create_cloud(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dddec7f",
   "metadata": {},
   "source": [
    "Adding a new function to get rid off the spesific words that we encounter at the dataset most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebd2dd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_remove = [\"ga\", \"kind\", \"hello\", \"please\", \"let\", \"help\", \"best\", \"regards\", \"icon\", \"dear\", \"per\", \"hi\", \"thanks\", \"thank\", \"importance\", \"high\", \"issue\", \"ab\", \"abc\"]\n",
    "\n",
    "def remove_words(text):\n",
    "  return \" \".join([word for word in text.lower().split() if word not in words_to_remove])\n",
    "\n",
    "df[\"Document\"] = df[\"Document\"].apply(remove_words)\n",
    "#create_cloud(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be9fc4f",
   "metadata": {},
   "source": [
    "To see an example of a [\"Document] we create a new func becouse the original one is too long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dc5f205c06a17ae",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work experience user work experience user work experience student coming next name much appreciate duration\n",
      "Access\n"
     ]
    }
   ],
   "source": [
    "def see_example_row(index:int):\n",
    "    first_document = df[\"Document\"].iloc[index]\n",
    "    words = first_document.split()[:100]\n",
    "    print(\" \".join(words))\n",
    "    print(df[\"Topic_group\"].iloc[index])\n",
    "\n",
    "see_example_row(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "875669e1bfeecf49",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer(stop_words=&#x27;english&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(stop_words=&#x27;english&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer(stop_words='english')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "vectorizer.fit(df[\"Document\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6014e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix = vectorizer.transform(df[\"Document\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9853966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group documents by topic\n",
    "grouped_documents = {}\n",
    "for doc, topic in zip(df[\"Document\"], df[\"Topic_group\"]):\n",
    "  if topic not in grouped_documents:\n",
    "    grouped_documents[topic] = []\n",
    "  grouped_documents[topic].append(doc)\n",
    "grouped_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d92dceab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "** Topic Group: Hardware **\n",
      "connection setup engineers details needed lead\n",
      "\n",
      "** Topic Group: Access **\n",
      "work experience user work experience user work experience student coming next name much appreciate duration\n",
      "\n",
      "** Topic Group: Miscellaneous **\n",
      "mail verification warning got attached addresses monitoring analyst verification warning\n",
      "\n",
      "** Topic Group: HR Support **\n",
      "access request modules report report cost much\n",
      "\n",
      "** Topic Group: Purchase **\n",
      "system movement left available device device denmark copenhagen denmark source quotation shipping lead\n",
      "\n",
      "** Topic Group: Administrative rights **\n",
      "notification wireless devices upgrade cr medium wireless devices upgrade cr medium announce users window wireless senior engineer cr medium summary software upgrade release approved wait completed starting ref msg\n",
      "\n",
      "** Topic Group: Storage **\n",
      "mailbox almost full mailbox almost mailbox almost senior infrastructure engineer infrastructure upcoming holiday none id\n",
      "\n",
      "** Topic Group: Internal Project **\n",
      "opportunity pas known pipeline opportunity known pipeline import bellow opportunity known pipeline opportunity id officer\n"
     ]
    }
   ],
   "source": [
    "# Analyze TF-IDF for each topic group\n",
    "for topic, group_documents in grouped_documents.items():\n",
    "  print(f\"\\n** Topic Group: {topic} **\")\n",
    "  print((group_documents[0]))\n",
    "\n",
    "  # Create and fit TF-IDF vectorizer\n",
    "  group_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "  tfid_matrix = group_vectorizer.fit_transform(group_documents)\n",
    "  feature_names = group_vectorizer.get_feature_names_out()\n",
    "  \n",
    "  # Analyze overall TF-IDF for the topic group\n",
    "  overall_tfidf_sum = tfidf_matrix.sum(axis=0)\n",
    "  \n",
    "  overall_top_features = sorted(zip(feature_names, overall_tfidf_sum), key=lambda x: x[1], reverse=True)[:10]\n",
    "  #print(overall_top_features)\n",
    "\n",
    "  #print(pd.DataFrame(tfid_matrix.toarray(), columns = feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb480a7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
